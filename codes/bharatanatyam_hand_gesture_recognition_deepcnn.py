# -*- coding: utf-8 -*-
"""Bharatanatyam_hand_gesture_recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-vkti7rFUYxZYeHvQhWFQmcXJ2vJp2iT

# **HAND GESTURE RECOGNITION**

---

## Dataset Preparation
"""

from google.colab import drive
drive.mount('/content/drive')

!scp /content/drive/MyDrive/bht_mudra_hands.zip ./
!unzip bht_mudra_hands

path = "/content/bht_mudra_hands/"

import shutil
import os

files = os.listdir(path)
# print(files)
mudras=[]
for file in files:
  mudras.append(file.split('_')[0])

mudras=list(set(mudras))
m1=[]
for mudra in mudras:
  m1.append(mudra.strip())
mudras=m1
mudras.sort()
print(mudras)

single_hand=['Pathaka', 'Tripathaka','Ardhapathaka','Mayura','Katrimukha','Ardhachandran','Aralam',
             'Shukatundam','Mushti','Sikharam','Kapith','Katakamukha 1','Katakamukha 2','Katakamukha 3',
             'Suchi','Chandrakala','Padmakosha','Sarpasirsha','Mrigasirsha','Simhamukham','Kangulam',
             'Alapadmam','Mukulam','Chaturam','Bramaram','Hamsasyam','Hamsapaksha','Tamarachudam','Trishulam']

double_hand=['Anjali','Kapotham','Karkatta','Swastikam','Pushpaputa','Shivalinga','Katakavardhana',
             'Kartariswastika','Sakata','Shanka','Chakra','Samputa','Pasha','Kilaka','Matsya','Kurma',
             'Varaha','Garuda','Nagabandha','Khatva','Berunda']    

# print(len(single_hand)+len(double_hand))

hand_map=dict()

f=0
for i in single_hand:  #0-28 for single hand
  hand_map[i]=f
  f+=1

for i in double_hand: # 29-49 for double hand
  hand_map[i]=f
  f+=1
print(hand_map)

dataset=[]
# for i in range(50):
#   dataset.append([])

for file in os.listdir(path):
  # try:
    f_name=file.split('_')[0]
    f_name=f_name.strip()
    f_name=f_name.replace('\u200b','')
    # f_name=f_name.strip('\u200b')
    # dataset[hand_map[f_name]].append([f_name,path+file])
    dataset.append([f_name,path+file])


print(dataset)

"""## Prepare Train and Test set"""

import pandas as pd
import numpy as np


Mudras_df = pd.DataFrame(dataset)
Mudras_df.columns = ['mudra', 'path']


    
print('Bharatnatayam Mudra datasets')
Mudras_df.head()

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split


mudra_X = Mudras_df['path'].values
mudra_Y = Mudras_df['mudra'].values

# print(mudra_X)
# print(mudra_Y)

x_train,x_test,y_train,y_test=train_test_split(mudra_X,mudra_Y,random_state=0,test_size=0.20,shuffle=True)

print(x_train)
print(x_test)
print(y_train)
print(y_test)

"""## Perform SkinMasking"""

import cv2

def s_mask(path):    
    frame = cv2.imread(path)
    frame = cv2.resize(frame,(96,96))
    # downsize it to reduce processing time
    #cv2.imshow("original",frame)
    converted = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) # Convert from RGB to HSV
    #print(frame.shape)
    #tuned settings
    lowerBoundary = np.array([0,40,30],dtype="uint8")
    upperBoundary = np.array([43,255,254],dtype="uint8")

    skinMask = cv2.inRange(converted, lowerBoundary, upperBoundary)

    # apply a series of erosions and dilations to the mask using an elliptical kernel
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))
    skinMask = cv2.erode(skinMask, kernel, iterations = 2)
    skinMask = cv2.dilate(skinMask, kernel, iterations = 2)

    lowerBoundary = np.array([170,80,30],dtype="uint8")
    upperBoundary = np.array([180,255,250],dtype="uint8")

    skinMask2 = cv2.inRange(converted, lowerBoundary, upperBoundary)
    skinMask = cv2.addWeighted(skinMask,0.5,skinMask2,0.5,0.0)
    #print(skinMask.flatten())
    #print(skinMask.shape)

    # blur the mask to help remove noise, then apply the
    # mask to the frame
    skinMask = cv2.medianBlur(skinMask, 5)
    skin = cv2.bitwise_and(frame, frame, mask = skinMask)
    frame = cv2.addWeighted(frame,1.5,skin,-0.5,0)
    skin = cv2.bitwise_and(frame, frame, mask = skinMask)

    #cv2.imshow("masked",skin) # Everything apart from skin is shown to be black

    h,w = skin.shape[:2]
    bw_image = cv2.cvtColor(skin, cv2.COLOR_HSV2BGR)  # Convert image from HSV to BGR format
    bw_image = cv2.cvtColor(skin, cv2.COLOR_BGR2GRAY)  # Convert image from BGR to gray format
    bw_image = cv2.GaussianBlur(bw_image,(5,5),0)  # Highlight the main object
    threshold = 1
    for i in range(h):
        for j in range(w):
            if bw_image[i][j] > threshold:
               bw_image[i][j] = 0
            else:
               bw_image[i][j] = 255


    #cv2.imshow("thresholded",bw_image)
    #cv2.waitKey(0)
    cv2.destroyAllWindows()
    return bw_image

from google.colab.patches import cv2_imshow
masked_image=s_mask(dataset[0][1])
flattened_sign_image=masked_image.flatten()

img = cv2.imread(dataset[0][1], cv2.IMREAD_UNCHANGED)
cv2_imshow(img)

# img = cv2.imread(masked_image, cv2.IMREAD_UNCHANGED)
cv2_imshow(masked_image)

min_YCrCb = np.array([0,133,77],np.uint8)
max_YCrCb = np.array([235,173,127],np.uint8)
image = cv2.imread(dataset[0][1])
imageYCrCb = cv2.cvtColor(image,cv2.COLOR_BGR2YCR_CB)
skinRegionYCrCb = cv2.inRange(imageYCrCb,min_YCrCb,max_YCrCb)
skinYCrCb = cv2.bitwise_and(image, image, mask = skinRegionYCrCb)
cv2_imshow(skinYCrCb)

image = cv2.imread(dataset[3700][1])
imageYCrCb = cv2.cvtColor(image,cv2.COLOR_BGR2YCR_CB)
skinRegionYCrCb = cv2.inRange(imageYCrCb,min_YCrCb,max_YCrCb)
skinYCrCb = cv2.bitwise_and(image, image, mask = skinRegionYCrCb)
cv2_imshow(skinYCrCb)

from enum import Enum, auto
import cv2
import numpy as np


class Method(Enum):
	'''Available methods for processing an image
			REGION_BASED: segment the skin using the HSV and YCbCr colorspaces, followed by the Watershed algorithm'''
	REGION_BASED = auto()


class SkinDetector():

	def __init__(self, image_path: str) -> None:
		self.image = cv2.imread(image_path)
		self.image_mask = None
		self.skin = None

	def find_skin(self, method=Method.REGION_BASED) -> None:
		'''function to process the image based on some method '''
		if (method == Method.REGION_BASED):
			self.__color_segmentation()
			self.__region_based_segmentation()

	def get_resulting_images(self) -> tuple:
		"""Returns the processed images
				[0] = The original image
				[1] = The resulting image mask containing the skin
				[2] = The result image after a bitwise_and of [1] and [0]"""

		return self.image, self.image_mask, self.skin

	def __color_segmentation(self) -> None:
		'''Apply a threshold to an HSV and YCbCr images, the used values were based on current research papers along with some empirical tests and visual evaluation'''

		HSV_image = cv2.cvtColor(self.image, cv2.COLOR_BGR2HSV)
		YCbCr_image = cv2.cvtColor(self.image, cv2.COLOR_BGR2YCR_CB)

		lower_HSV_values = np.array([0, 40, 0], dtype="uint8")
		upper_HSV_values = np.array([25, 255, 255], dtype="uint8")

		lower_YCbCr_values = np.array((0, 138, 67), dtype="uint8")
		upper_YCbCr_values = np.array((255, 173, 133), dtype="uint8")

		# A binary mask is returned. White pixels (255) represent pixels that fall into the upper/lower.
		mask_YCbCr = cv2.inRange(YCbCr_image, lower_YCbCr_values, upper_YCbCr_values)
		mask_HSV = cv2.inRange(HSV_image, lower_HSV_values, upper_HSV_values)

		self.skin_mask = cv2.add(mask_HSV, mask_YCbCr)

	def __region_based_segmentation(self) -> None:
		'''Function that applies Watershed and morphological operations on the thresholded image morphological operations'''

		image_foreground = cv2.erode(self.skin_mask, None, iterations=3)  # remove noise

		# The background region is reduced a little because of the dilate operation
		dilated_binary_image = cv2.dilate(self.skin_mask, None, iterations=3)
		# set all background regions to 128
		_, image_background = cv2.threshold(dilated_binary_image, 1, 128, cv2.THRESH_BINARY)

		# add both foreground and background, forming markers. The markers are "seeds" of the future image regions.
		image_marker = cv2.add(image_foreground, image_background)
		image_marker32 = np.int32(image_marker)  # convert to 32SC1 format

		image_marker32 = cv2.watershed(self.image, image_marker32)
		m = cv2.convertScaleAbs(image_marker32)  # convert back to uint8

		# bitwise of the mask with the input image
		_, self.image_mask = cv2.threshold(m, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
		self.skin = cv2.bitwise_and(self.image, self.image, mask=self.image_mask)

	def show_all_images(self, window_title="Original Image | Skin Mask | Result") -> None:
		'''Show all processed images concatenated along the 1 axis using imshow '''
		rgb_mask = cv2.cvtColor(self.image_mask, cv2.COLOR_GRAY2RGB)
		cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)
		cv2.imshow(window_title, np.concatenate((self.image, rgb_mask, self.skin), axis=1))
		cv2.waitKey(0)

detector = SkinDetector(image_path = dataset[0][1])
detector.find_skin()
detector.show_all_images()

"""## Model Training"""

import cv2
X=[]
Y=[]
for path,label in zip(mudra_X,mudra_Y):
  img=cv2.imread(path) #reads image and return np array
  img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
  img=cv2.resize(img,(320,120))
  X.append(img)
  Y.append(hand_map[label])

# print(X)
# print(Y)

print(len(X[0]),len(X))
print(Y)

X=np.array(X,dtype="uint8")
X=X.reshape(len(mudra_X),120,320,1)
Y=np.array(Y)

print(len(X))
print(Y)

from sklearn.model_selection import train_test_split
# X = pd.DataFrame(X)
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=42)

from keras.models import Sequential
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.layers import Dense, Flatten

model = Sequential()
model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(120, 320, 1))) 
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu')) 
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(50, activation='softmax'))



# Configures the model for training
model.compile(optimizer='adam', # Optimization routine, which tells the computer how to adjust the parameter values to minimize the loss function.
              loss='sparse_categorical_crossentropy', # Loss function, which tells us how bad our predictions are.
              metrics=['accuracy']) # List of metrics to be evaluated by the model during training and testing.

# Trains the model for a given number of epochs (iterations on a dataset) and validates it.
model.fit(X_train, Y_train, epochs=10, batch_size=256, verbose=2, validation_data=(X_test, Y_test))

model.save('handrecognition_model.h5')

test_loss, test_acc = model.evaluate(X_test, Y_test)

print('Test accuracy: {:2.2f}%'.format(test_acc*100))

predictions = model.predict(X_test)

np.argmax(predictions[0]), Y_test[0]

hand_map_reverse=dict()

for key,value in hand_map.items():
  hand_map_reverse[value]=key

print(hand_map_reverse)

# Commented out IPython magic to ensure Python compatibility.
from sklearn.metrics import confusion_matrix # Helps present results as a confusion-matrix
import matplotlib.pyplot as plt
# %matplotlib inline
# Function to plot images and labels for validation purposes
def validate_60_images(predictions_array, true_label_array, img_array):
  # Array for pretty printing and then figure size
  class_names = ["down", "palm", "l", "fist", "fist_moved", "thumb", "index", "ok", "palm_moved", "c"] 
  plt.figure(figsize=(100,100))
  
  for i in range(1, 61):
    # Just assigning variables
    prediction = predictions_array[i]
    true_label = true_label_array[i]
    img = img_array[i]
    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
    
    # Plot in a good way
    plt.subplot(20,3,i)
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(img, cmap=plt.cm.binary)

    predicted_label = np.argmax(prediction) # Get index of the predicted label from prediction
    
    # Change color of title based on good prediction or not
    if predicted_label == true_label:
      color = 'blue'
    else:
      color = 'red'

    plt.xlabel("Predicted: {} {:2.0f}% (True: {})".format(hand_map_reverse[predicted_label],
                                  100*np.max(prediction),
                                  hand_map_reverse[true_label]),
                                  color=color)
  plt.show()

validate_60_images(predictions, Y_test, X_test)

y_pred = np.argmax(predictions, axis=1)

pd.DataFrame(confusion_matrix(y_test, y_pred))